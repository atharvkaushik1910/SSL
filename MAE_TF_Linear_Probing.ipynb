{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b31f438f",
   "metadata": {},
   "source": [
    "# üîç Linear Probing with Pretrained MAE Encoder - TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00844d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c019936d",
   "metadata": {},
   "source": [
    "## üì¶ Load Labeled Dataset for Linear Probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7318b7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "NUM_CLASSES = 100  # Adjust to match ImageNet-100\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    \"path_to/train\",\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE)\n",
    ").map(lambda x, y: (x / 255.0, y))\n",
    "\n",
    "val_dataset = image_dataset_from_directory(\n",
    "    \"path_to/val\",\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE)\n",
    ").map(lambda x, y: (x / 255.0, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d96d93",
   "metadata": {},
   "source": [
    "## üî≤ Patchify Function for Embedding Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aff9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def patchify(images, patch_size=16):\n",
    "    batch_size = tf.shape(images)[0]\n",
    "    patches = tf.image.extract_patches(\n",
    "        images=images,\n",
    "        sizes=[1, patch_size, patch_size, 1],\n",
    "        strides=[1, patch_size, patch_size, 1],\n",
    "        rates=[1, 1, 1, 1],\n",
    "        padding='VALID'\n",
    "    )\n",
    "    patch_dims = patches.shape[-1]\n",
    "    patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "    return patches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34a3572",
   "metadata": {},
   "source": [
    "## üîÅ Load Pretrained Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18269089",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_encoder(input_shape, num_patches, embed_dim):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Dense(embed_dim)(inputs)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    for _ in range(4):\n",
    "        x1 = layers.LayerNormalization()(x)\n",
    "        x1 = layers.MultiHeadAttention(num_heads=4, key_dim=embed_dim)(x1, x1)\n",
    "        x = layers.Add()([x, x1])\n",
    "    outputs = layers.LayerNormalization()(x)\n",
    "    return models.Model(inputs, outputs, name=\"encoder\")\n",
    "\n",
    "PATCH_SIZE = 16\n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
    "PATCH_DIM = PATCH_SIZE * PATCH_SIZE * 3\n",
    "EMBED_DIM = 128\n",
    "\n",
    "encoder = create_encoder((NUM_PATCHES, PATCH_DIM), NUM_PATCHES, EMBED_DIM)\n",
    "encoder.load_weights(\"mae_encoder_tf.h5\")\n",
    "encoder.trainable = False  # Freeze the encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbe3792",
   "metadata": {},
   "source": [
    "## üß† Linear Classifier on Top of Frozen Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7f5274",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "patches = patchify(inputs)\n",
    "features = encoder(patches)\n",
    "features = layers.GlobalAveragePooling1D()(features)\n",
    "outputs = layers.Dense(NUM_CLASSES, activation='softmax')(features)\n",
    "\n",
    "model = models.Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93e224c",
   "metadata": {},
   "source": [
    "## üöÄ Train Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8e1f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7698f836",
   "metadata": {},
   "source": [
    "## üìä Evaluation: Accuracy and F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3308e385",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Get true and predicted labels\n",
    "y_true, y_pred = [], []\n",
    "for x_batch, y_batch in val_dataset:\n",
    "    preds = model.predict(x_batch)\n",
    "    y_true.extend(y_batch.numpy())\n",
    "    y_pred.extend(np.argmax(preds, axis=1))\n",
    "\n",
    "print(classification_report(y_true, y_pred, digits=4))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
